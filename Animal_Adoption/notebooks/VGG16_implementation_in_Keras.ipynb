{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e809e910-22e4-4293-a6d9-db98fc8d7382",
   "metadata": {},
   "source": [
    "## VGG16 implementation in Keras \n",
    "- https://towardsdatascience.com/step-by-step-vgg16-implementation-in-keras-for-beginners-a833c686ae6c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79550b61-8a3a-4152-935a-cf4fd4f5c74a",
   "metadata": {},
   "source": [
    "- import all the libraries needed to implement VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10c37169-61b9-4637-a7ab-b72bba410e31",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2aa88478-29d2-48db-8882-28f6a3e15140",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras,os\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPool2D , Flatten\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6dde12-2492-4085-8bd4-2ecc13c9dec5",
   "metadata": {},
   "source": [
    "LOAD the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7855b91-c2bd-4724-a697-003646f8923b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ae41f7f-0890-4d34-ace3-f76b16238d9b",
   "metadata": {},
   "source": [
    "- split into train- and test-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4eb216f-57a4-4ce0-a1e6-369098433837",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ImageDataGenerator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trdata \u001b[38;5;241m=\u001b[39m \u001b[43mImageDataGenerator\u001b[49m()\n\u001b[1;32m      2\u001b[0m traindata \u001b[38;5;241m=\u001b[39m trdata\u001b[38;5;241m.\u001b[39mflow_from_directory(directory\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m,target_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m224\u001b[39m,\u001b[38;5;241m224\u001b[39m))\n\u001b[1;32m      3\u001b[0m tsdata \u001b[38;5;241m=\u001b[39m ImageDataGenerator()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ImageDataGenerator' is not defined"
     ]
    }
   ],
   "source": [
    "trdata = ImageDataGenerator()\n",
    "traindata = trdata.flow_from_directory(directory=\"data\",target_size=(224,224))\n",
    "tsdata = ImageDataGenerator()\n",
    "testdata = tsdata.flow_from_directory(directory=\"test\", target_size=(224,224))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e281b243-aa77-4b37-9d94-b866fea75b1b",
   "metadata": {},
   "source": [
    "The ImageDataGenerator will automatically label all the data inside cat folder as cat and vis-Ã -vis for dog folder. In this way data is easily ready to be passed to the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e688de-9b9b-4991-bb98-b19855eee128",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(input_shape=(224,224,3),filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679a483c-e922-4d7d-aec4-f630b5a4e254",
   "metadata": {},
   "source": [
    "- description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac56a71-5a9f-43c4-a6a8-084f96fce856",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Flatten())\n",
    "model.add(Dense(units=4096,activation=\"relu\"))\n",
    "model.add(Dense(units=4096,activation=\"relu\"))\n",
    "model.add(Dense(units=2, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e65efe9-705c-4b47-91b9-6a0c44831da5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddecfde7-1681-4829-905c-b49f2fc6e028",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "opt = Adam(lr=0.001)\n",
    "model.compile(optimizer=opt, loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22eb4b9a-9aea-4351-9792-621e173f6abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48e4305-da5d-4772-aae3-463fcaffa2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "checkpoint = ModelCheckpoint(\"vgg16_1.h5\", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "early = EarlyStopping(monitor='val_acc', min_delta=0, patience=20, verbose=1, mode='auto')\n",
    "hist = model.fit_generator(steps_per_epoch=100,generator=traindata, validation_data= testdata, validation_steps=10,epochs=100,callbacks=[checkpoint,early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dd6e81-0a17-4cb6-9fb5-d6e31eddeaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(hist.history[\"acc\"])\n",
    "plt.plot(hist.history['val_acc'])\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title(\"model accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend([\"Accuracy\",\"Validation Accuracy\",\"loss\",\"Validation Loss\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c70490-021e-458f-b68d-5a914863c7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "img = image.load_img(\"image.jpeg\",target_size=(224,224))\n",
    "img = np.asarray(img)\n",
    "plt.imshow(img)\n",
    "img = np.expand_dims(img, axis=0)\n",
    "from keras.models import load_model\n",
    "saved_model = load_model(\"vgg16_1.h5\")\n",
    "output = saved_model.predict(img)\n",
    "if output[0][0] > output[0][1]:\n",
    "    print(\"cat\")\n",
    "else:\n",
    "    print('dog')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbbe5d8-f42e-4600-bb3e-0bd0e03bba14",
   "metadata": {},
   "source": [
    "## Adapte on our task\n",
    "- https://www.learndatasci.com/tutorials/hands-on-transfer-learning-keras/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afc941e-7a8e-4cac-9701-6ab1680f6cd0",
   "metadata": {},
   "source": [
    "So we'll import a pre-trained model like VGG16, but \"cut off\" the Fully-Connected layer - also called the \"top\" model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7f81a43-3e57-4bf2-95e8-f1127a849687",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Adam\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapplications\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvgg16\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VGG16, preprocess_input\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from pathlib import Path\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db18257-ce3a-4b7b-b3fc-0cb54a14dffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "train_generator = ImageDataGenerator(rotation_range=90, \n",
    "                                     brightness_range=[0.1, 0.7],\n",
    "                                     width_shift_range=0.5, \n",
    "                                     height_shift_range=0.5,\n",
    "                                     horizontal_flip=True, \n",
    "                                     vertical_flip=True,\n",
    "                                     validation_split=0.15,\n",
    "                                     preprocessing_function=preprocess_input) # VGG16 preprocessing\n",
    "\n",
    "test_generator = ImageDataGenerator(preprocessing_function=preprocess_input) # VGG16 preprocessing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
